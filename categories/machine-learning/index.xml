<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning on Diego Calanzone</title>
    <link>https://halixness.github.io/categories/machine-learning/</link>
    <description>Recent content in machine learning on Diego Calanzone</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Â© Diego Calanzone 2021</copyright>
    <lastBuildDate>Tue, 28 Dec 2021 17:42:01 +0100</lastBuildDate><atom:link href="https://halixness.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>An overview of the latest progress in Generative Diffusion Models</title>
      <link>https://halixness.github.io/posts/overview-diffusion-models/overview-diffusion-models/</link>
      <pubDate>Tue, 28 Dec 2021 17:42:01 +0100</pubDate>
      
      <guid>https://halixness.github.io/posts/overview-diffusion-models/overview-diffusion-models/</guid>
      <description>Langevin Dynamics, Score-Based Generative Models Our generation has a philosopher. He is not an artist, or a professional writer. He is a programmer. )Based on the Metropolis-Hastings algorithm: idea: use a markov chain (process of sequential random variables) whose equilibrium distribution approximates the desired one algorithm: define a proposed approximating distribution g(x). \(x^* = x_t + \mathcal{N}(0, \sigma^2 I * f\left(u\right))\) . Compute \(A=\min \left(1, \frac{\pi(y) Q\left(x_t \mid y\right)}{\pi\left(x_t\right) Q\left(y \mid x_t\right)}\right)\), with probability \(A\) we accept \(x_{t-1} = x^\), otherwise keep \(x_{t-1} = x_t$.</description>
    </item>
    
  </channel>
</rss>
