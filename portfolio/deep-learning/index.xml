<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep-learning on Diego Calanzone</title>
    <link>https://halixness.github.io/portfolio/deep-learning/</link>
    <description>Recent content in deep-learning on Diego Calanzone</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© Diego Calanzone 2021</copyright><atom:link href="https://halixness.github.io/portfolio/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>deepSWE: video prediction applied on shallow water equations.</title>
      <link>https://halixness.github.io/projects/deep-learning/deepswe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://halixness.github.io/projects/deep-learning/deepswe/</guid>
      <description>SourceCodeBetween late 2020 and summer 2021, I had the pleasure to work with Prof. Alessandro Dal Palú and the HyLab research team at the University of Parma. The subject for this thesis project is Shallow Water Equations and methods in deep learning to compute them with cost and time efficiency.
My proposal was an initial approach to video prediction models: the flooding scenario is discretized into a 2D grid with variable resolution.</description>
    </item>
    
    <item>
      <title>Learning Transferable Visual Models From Natural Language Supervision</title>
      <link>https://halixness.github.io/projects/deep-learning/clip-seminar/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://halixness.github.io/projects/deep-learning/clip-seminar/</guid>
      <description>TL;DR: you can find the repository here.
In october 2021 I joined the seminar &amp;ldquo;Learning with Limited labeled data&amp;rdquo; at the University of Tuebingen. I decided to study and present the model &amp;ldquo;CLIP&amp;rdquo; and correlated: DALL-E, Big Sleep, CLIP GLaSS.
Briefly, in this paper a novel approach for natural language vision is proposed: in order to link text with images, captions and photos are encoded as vectors in the same embedding space, with contrastive learning the model learns to best match these vectors.</description>
    </item>
    
  </channel>
</rss>
