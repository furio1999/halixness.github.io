<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>Learning Transferable Visual Models From Natural Language Supervision</title><link rel=stylesheet href=https://halixness.github.io/css/colors-gray.min.384cc912903f8bd71ef28886d9740896b61bad168977dace7ace1b7d18bfe1db.css><link rel=icon href=https://halixness.github.io/favicon.ico></head><body><section id=contained-engraving></section><header id=header><h1><a href=https://halixness.github.io>Diego Calanzone</a></h1><p class=description>Deep learning student, software engineer. Made in üáÆüáπ.<br>(refresh to see many üêª)</p></header><div id=page><div id=sidebar><nav><hr class=menu-line><ul class=nav><li><a href=https://halixness.github.io/about><span>About</span></a></li><li><a href=https://halixness.github.io/><span>Blog</span></a></li><li><a href=https://halixness.github.io/projects><span>Projects</span></a></li></ul><hr class=menu-line></nav></div><div id=content><article class=post><h1 class=single-title><a href=https://halixness.github.io/projects/deep-learning/clip-seminar/>Learning Transferable Visual Models From Natural Language Supervision</a></h1><div class="single-content post-content"><p><img src=https://halixness.github.io/images/posts/31-12-2021/clip-seminar.png alt="Presentation home"></p><p><strong>TL;DR</strong>: you can find the repository <a href=https://github.com/halixness/understanding-CLIP>here</a>.</p><p>In october 2021 I joined the seminar &ldquo;Learning with Limited labeled data&rdquo; at the University of Tuebingen. I decided to study and present the model &ldquo;CLIP&rdquo; and correlated: DALL-E, Big Sleep, CLIP GLaSS.</p><p>Briefly, in this paper a novel approach for natural language vision is proposed: in order to link text with images, captions and photos are encoded as vectors in the same embedding space, with contrastive learning the model learns to best match these vectors. The potential is limitless: CLIP is trained on a large web-based dataset (WIT, 400 millions of samples) and it&rsquo;s capable of transfering such knowledge to a multitude of downstream tasks (classification, captioning) with astonishing performance on data never seen before.</p><p>You can check my repository for more detailed notes, notebooks and a carefully crafted presentation! :)</p></div></article></div><footer id=footer><p class=copyright>¬© Diego Calanzone 2021</p></footer></div><script async src="https://www.googletagmanager.com/gtag/js?id=UA-216110268-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-216110268-1');</script><script type=text/javascript>engravings=6;index=Math.floor(Math.random()*(engravings));document.getElementById("contained-engraving").style.backgroundImage="url('/images/engravings/"+index+".svg')";</script></body></html>